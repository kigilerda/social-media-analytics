{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67b4950",
   "metadata": {},
   "source": [
    "# Часть 3. Векторизация текста\n",
    "\n",
    "**Цель:** преобразовать предобработанные тексты в числовые признаки для последующего обучения моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0f08cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c485ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11eb8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = DATA_DIR / \"data_train.csv\"\n",
    "val_path   = DATA_DIR / \"data_val.csv\"\n",
    "test_path  = DATA_DIR / \"data_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c0a346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_path)\n",
    "val   = pd.read_csv(val_path)\n",
    "test  = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55cbad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (train, val, test):\n",
    "    df[\"processed_text\"] = df[\"processed_text\"].fillna(\"\").astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4ee6f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3630, 14), (428, 14), (503, 14))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e42c53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = train[\"processed_text\"].values\n",
    "X_val_text   = val[\"processed_text\"].values\n",
    "X_test_text  = test[\"processed_text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0669974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[\"target\"].values if \"target\" in train.columns else None\n",
    "y_val   = val[\"target\"].values if \"target\" in val.columns else None\n",
    "y_test  = test[\"target\"].values if \"target\" in test.columns else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf9cb124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3630, 428, 503)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_text), len(X_val_text), len(X_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b33a2f",
   "metadata": {},
   "source": [
    "## BoW\n",
    "BoW формирует вектор признаков как частоты слов, без учета порядка\n",
    "Используем только обучающую выборку для обучения векторизатора, затем применяем к val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6bf3c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7be9684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(\n",
    "    min_df=2,          # можно подкрутить, чтобы убрать редкие слова\n",
    "    max_df=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe798a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow = bow.fit_transform(X_train_text)\n",
    "X_val_bow   = bow.transform(X_val_text)\n",
    "X_test_bow  = bow.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b8a4ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7942"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61c1b1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3630, 7942), (428, 7942), (503, 7942))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow.shape, X_val_bow.shape, X_test_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dfa153",
   "metadata": {},
   "source": [
    "Сохранение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71cfea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VEC_DIR  = Path(\"../models/vectorizers\")\n",
    "FEAT_DIR = Path(\"../data/features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5eada59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\models\\\\vectorizers\\\\bow.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(bow, VEC_DIR / \"bow.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0ec585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(FEAT_DIR / \"X_train_bow.npz\", X_train_bow)\n",
    "sparse.save_npz(FEAT_DIR / \"X_val_bow.npz\",   X_val_bow)\n",
    "sparse.save_npz(FEAT_DIR / \"X_test_bow.npz\",  X_test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574fc773",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "TF-IDF снижает вклад слов, которые встречаются часто, и повышает вклад информативных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d42d41da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99c51e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    ngram_range=(1, 2)  # биграммы часто дают буст на текстах\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90343b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "X_val_tfidf   = tfidf.transform(X_val_text)\n",
    "X_test_tfidf  = tfidf.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d547e779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27571"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e0002f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3630, 27571)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb99485",
   "metadata": {},
   "source": [
    "Сохранение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cb8763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(tfidf, VEC_DIR / \"tfidf.joblib\")\n",
    "\n",
    "sparse.save_npz(FEAT_DIR / \"X_train_tfidf.npz\", X_train_tfidf)\n",
    "sparse.save_npz(FEAT_DIR / \"X_val_tfidf.npz\",   X_val_tfidf)\n",
    "sparse.save_npz(FEAT_DIR / \"X_test_tfidf.npz\",  X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c317690",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Word2Vec обучается на корпусе текстов и строит вектор для каждого слова.\n",
    "Для получения вектора текста используется усреднение векторов его слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c34fd849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09e21aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokens = [t.split() for t in X_train_text]\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=corpus_tokens,\n",
    "    vector_size=300,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    epochs=30,\n",
    "    sg=1  # skip-gram\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80d2a96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8271, 300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.wv.key_to_index), w2v_model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ae64757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v(texts, model):\n",
    "    dim = model.vector_size\n",
    "    out = np.zeros((len(texts), dim), dtype=np.float32)\n",
    "\n",
    "    for i, txt in enumerate(texts):\n",
    "        tokens = txt.split()\n",
    "        vecs = [model.wv[w] for w in tokens if w in model.wv]\n",
    "        if vecs:\n",
    "            out[i] = np.mean(vecs, axis=0)\n",
    "        else:\n",
    "            out[i] = np.zeros(dim, dtype=np.float32)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "495c224e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3630, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_w2v = w2v(X_train_text, w2v_model)\n",
    "X_val_w2v   = w2v(X_val_text, w2v_model)\n",
    "X_test_w2v  = w2v(X_test_text, w2v_model)\n",
    "\n",
    "X_train_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7420b98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.17977998 -0.05201431  0.7299367  -0.16983773  0.12864201 -0.18763827\n",
      "  0.6392232   0.4573925   0.35686222 -0.13720316 -0.3028439  -0.22199935\n",
      " -0.6007448  -0.32712448 -0.49208674 -0.27370358  0.01200585 -0.2498504\n",
      " -0.06460121  0.07269163 -0.52926236  0.0209351  -0.04519065  0.3709578\n",
      "  0.25904587 -0.5009567  -0.09983045  0.50056744  0.07983129  0.26324806\n",
      "  0.45939967  0.05264655  0.04747412 -0.37085444 -0.00581874 -0.06841763\n",
      "  0.4499923  -0.19635087  0.00323013  0.25756392 -0.650694    0.01219968\n",
      " -0.12714562 -0.572049   -0.09764056  0.30051318  0.2741736   0.26244625\n",
      " -0.0860332   0.31298673 -0.31466112 -0.50458306 -0.45976683 -0.20944677\n",
      " -0.32475707  0.30947408  0.16227537  0.2280456  -0.06464218 -0.04549627\n",
      "  0.18980214 -0.33818457  0.5314199   0.16144541  0.11750107 -0.26347956\n",
      "  0.26724496 -0.1492689  -0.14443454  0.06898117  0.10876758  0.00506366\n",
      " -0.18039288  0.08966258 -0.00503675  0.15507908  0.24569005  0.0331107\n",
      "  0.16734979 -0.20054838 -0.24379341  0.28085408  0.37632075  0.7683642\n",
      " -0.13212511  0.14846803 -0.26115525 -0.12409995 -0.01677624  0.37985873\n",
      " -0.19304682 -0.36647123  0.20059666 -0.5395032   0.15333726  0.21976718\n",
      "  0.1915069   0.19237207 -0.32152477  0.2812177   0.04661093 -0.06531607\n",
      "  0.04005456  0.04863351  0.2503186   0.05985593 -0.18826419  0.13772014\n",
      " -0.46864802  0.01704959  0.00525211 -0.40599936 -0.36885634 -0.30702707\n",
      "  0.18465696 -0.02521541  0.72784203 -0.13749479 -0.01750109 -0.20493887\n",
      "  0.24329711  0.18903112  0.66428053  0.6843262  -0.21882181 -0.23045056\n",
      "  0.3747251   0.02446451 -0.5104913  -0.12657769 -0.20135085 -0.13992134\n",
      "  0.25484666  0.16656737  0.05264121 -0.19636239  0.2000371  -0.6861259\n",
      " -0.14249673 -0.3425211   0.09589474 -0.33829448 -0.04535396  0.624378\n",
      "  0.3862011   0.33394188 -0.30644888 -0.06556283 -0.21016411 -0.3936224\n",
      "  0.26766542 -0.30282927 -0.58781785 -0.56907964  0.08223727  0.02280581\n",
      " -0.296169   -0.10765171  0.18345395  0.7092181  -0.44340262 -0.16483481\n",
      "  0.02181041  0.42251357  0.11795273 -0.6952261   0.6726528  -0.45778432\n",
      "  0.09297758  0.41477698 -0.47417375  0.16101526 -0.33571416  0.16117643\n",
      " -0.37155548  0.07089851  0.2808854  -0.39883885 -0.14842294  0.09736869\n",
      " -0.2691301  -0.08576585  0.19643997 -0.33436087  0.07350619  0.322961\n",
      " -0.34827918  0.4578019  -0.08135326 -0.40070972 -0.17315069 -0.20359673\n",
      " -0.5552074  -0.36060697 -0.05764612  0.0799133  -0.19085334 -0.08873439\n",
      "  0.06549936  0.05444926  0.04560427 -0.41443837 -0.3169362  -0.33395302\n",
      "  0.07755758  0.26228383 -0.03143038  0.23948933  0.2926875   0.30188835\n",
      "  0.39595273 -0.11830724 -0.21811765  0.2406508  -0.03238549  0.14045796\n",
      " -0.15493445 -0.6879765  -0.21820143 -0.21378668  0.10085602 -0.07570958\n",
      " -0.31950605 -0.09902573  0.11277138 -0.36206138  0.13870144 -0.5190175\n",
      " -0.09216004 -0.05061006  0.7339364   0.15935643  0.5887695  -0.5019899\n",
      "  0.10289185 -0.13436608  0.3371606  -0.25648162 -0.027985    0.4002688\n",
      " -0.3863062  -0.36911216 -0.18129998  0.17844935  0.2621526  -0.35670552\n",
      "  0.00897306 -0.1985223  -0.4551674   0.69175893 -0.06428026  0.7077802\n",
      "  0.39578885 -0.3237607   0.2794307   0.31114635  0.5580402   0.08459126\n",
      "  0.07905825 -0.48279205  0.27991593  0.28431195  0.38742048 -0.17488955\n",
      "  0.10953256  0.7046997   0.02726164  0.24973848  0.22875151 -0.02075222\n",
      " -0.13610789 -0.10882876  0.25436264 -0.18392642  0.14696391 -0.46421626\n",
      "  0.46982235  0.22317183 -0.39901072 -0.09552412  0.31764656  0.638957\n",
      " -0.40398526  0.5430391   0.21387653 -0.7943261  -0.47497442  0.15081495\n",
      "  0.04629669  0.3944164  -0.3807686   0.48977464  0.52993304  0.5307212\n",
      "  0.5657525  -0.27397642  0.09418818  0.13049485  0.45021686 -0.39449918]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv['tesla'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52742a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('intelligence', 0.5937744379043579),\n",
       " ('homepod', 0.572170078754425),\n",
       " ('airtag', 0.5532761216163635),\n",
       " ('music', 0.4969641864299774),\n",
       " ('снести', 0.48946940898895264),\n",
       " ('шнур', 0.4827805161476135),\n",
       " ('tv', 0.4810587167739868),\n",
       " ('keyboard', 0.4790112376213074),\n",
       " ('генеалогия', 0.4738845229148865),\n",
       " ('передумать', 0.47298169136047363)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ближайшие слова\n",
    "w2v_model.wv.most_similar('apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f957023d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('шифроваться', 0.45820122957229614),\n",
       " ('яблочный', 0.4561236798763275),\n",
       " ('улавливать', 0.4410543143749237),\n",
       " ('мач', 0.4393463134765625),\n",
       " ('canon', 0.42983031272888184),\n",
       " ('подтверждаться', 0.4213932454586029),\n",
       " ('galaxy', 0.4164232015609741),\n",
       " ('ночник', 0.4155904948711395),\n",
       " ('прошивка', 0.41365525126457214),\n",
       " ('magsafe', 0.4036543071269989)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ближайшие слова\n",
    "w2v_model.wv.most_similar('смартфон')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76111e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.29898223)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# косинусная близость\n",
    "w2v_model.wv.similarity('apple', 'iphone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f5a5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIR  = Path(\"../models/embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cefe706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(str(EMB_DIR / \"word2vec.model\"))\n",
    "\n",
    "np.save(FEAT_DIR / \"X_train_w2v.npy\", X_train_w2v)\n",
    "np.save(FEAT_DIR / \"X_val_w2v.npy\",   X_val_w2v)\n",
    "np.save(FEAT_DIR / \"X_test_w2v.npy\",  X_test_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5142e32",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "Используем мультиязычную модель, чтобы корректно работать с русским текстом.\n",
    "Извлекаем вектор текста:\n",
    "- cls: вектор токена [CLS]\n",
    "- mean: среднее по всем токенам последнего слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30af70f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arina\\VS Code Projects\\social-media-analytics\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3b8bedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4edaf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-multilingual-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1ca23b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 303.56it/s, Materializing param=pooler.dense.weight]                               \n",
      "BertModel LOAD REPORT from: bert-base-multilingual-cased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert = BertModel.from_pretrained(MODEL_NAME).to(device)\n",
    "bert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1880563c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e1ae2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_embeddings(texts, batch_size=16, max_len=128, pooling=\"cls\"):\n",
    "    vecs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = list(texts[i:i+batch_size])\n",
    "\n",
    "            enc = tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            last_hidden = bert(**enc).last_hidden_state  # [B, T, H]\n",
    "\n",
    "            if pooling == \"cls\":\n",
    "                emb = last_hidden[:, 0, :]  # [B, H]\n",
    "            elif pooling == \"mean\":\n",
    "                mask = enc[\"attention_mask\"].unsqueeze(-1)  # [B, T, 1]\n",
    "                emb = (last_hidden * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "            else:\n",
    "                raise ValueError(\"pooling must be 'cls' or 'mean'\")\n",
    "\n",
    "            vecs.append(emb.cpu().numpy())\n",
    "\n",
    "    return np.vstack(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9134c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bert = bert_embeddings(X_train_text, pooling=\"cls\", batch_size=16, max_len=128)\n",
    "X_val_bert   = bert_embeddings(X_val_text,   pooling=\"cls\", batch_size=16, max_len=128)\n",
    "X_test_bert  = bert_embeddings(X_test_text,  pooling=\"cls\", batch_size=16, max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "339a8662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3630, 768) (428, 768) (503, 768)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_bert.shape, X_val_bert.shape, X_test_bert.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec0af904",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(FEAT_DIR / \"X_train_bert_cls.npy\", X_train_bert)\n",
    "np.save(FEAT_DIR / \"X_val_bert_cls.npy\",   X_val_bert)\n",
    "np.save(FEAT_DIR / \"X_test_bert_cls.npy\",  X_test_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b0e92",
   "metadata": {},
   "source": [
    "## Оценка тональности по тональному словарю \n",
    "\n",
    "Дополнительно оцениваем тональность текста поста по словарю/\n",
    "В качестве интерпретируемого базового признака использован русскоязычный тональный словарь RuSentiLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b262ed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEX_PATH = Path(\"../data/external/rusentilex.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5eb6c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rusentilex_txt(path):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Нет файла: {path.resolve()}\")\n",
    "\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"!\"):\n",
    "                continue\n",
    "            parts = [p.strip() for p in line.split(\",\")]\n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            rows.append(parts[:5])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"word\", \"pos\", \"lemma\", \"sentiment\", \"source\"])\n",
    "\n",
    "    df[\"lemma\"] = df[\"lemma\"].astype(str).str.lower().str.strip()\n",
    "    df[\"sentiment\"] = df[\"sentiment\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "    df = df[~df[\"lemma\"].str.contains(r\"\\s+\", regex=True)]\n",
    "    pos_set = set(df.loc[df[\"sentiment\"] == \"positive\", \"lemma\"])\n",
    "    neg_set = set(df.loc[df[\"sentiment\"] == \"negative\", \"lemma\"])\n",
    "\n",
    "    return pos_set, neg_set, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "221571d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: 2790 neg: 7867\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>lemma</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>аборт</td>\n",
       "      <td>Noun</td>\n",
       "      <td>аборт</td>\n",
       "      <td>negative</td>\n",
       "      <td>fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>абортивный</td>\n",
       "      <td>Adj</td>\n",
       "      <td>абортивный</td>\n",
       "      <td>negative</td>\n",
       "      <td>fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>абракадабра</td>\n",
       "      <td>Noun</td>\n",
       "      <td>абракадабра</td>\n",
       "      <td>negative</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>абсурд</td>\n",
       "      <td>Noun</td>\n",
       "      <td>абсурд</td>\n",
       "      <td>negative</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>абсурдность</td>\n",
       "      <td>Noun</td>\n",
       "      <td>абсурдность</td>\n",
       "      <td>negative</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word   pos        lemma sentiment   source\n",
       "0        аборт  Noun        аборт  negative     fact\n",
       "1   абортивный   Adj   абортивный  negative     fact\n",
       "2  абракадабра  Noun  абракадабра  negative  opinion\n",
       "3       абсурд  Noun       абсурд  negative  opinion\n",
       "4  абсурдность  Noun  абсурдность  negative  opinion"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_set, neg_set, rusenti_df = load_rusentilex_txt(LEX_PATH)\n",
    "\n",
    "print(\"pos:\", len(pos_set), \"neg:\", len(neg_set))\n",
    "rusenti_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d76ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicon_score(text, pos_set, neg_set):\n",
    "    toks = str(text).split() \n",
    "    if not toks:\n",
    "        return 0.0\n",
    "    pos_cnt = sum(t in pos_set for t in toks)\n",
    "    neg_cnt = sum(t in neg_set for t in toks)\n",
    "    return (pos_cnt - neg_cnt) / max(1, len(toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb09a3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_text</th>\n",
       "      <th>sent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hugging face выпустить бесплатный курс ия аген...</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>кастомизировать ес киберстандарт samsung блоки...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>банк россия снизить ключевой ставка это четвёр...</td>\n",
       "      <td>-0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>топ хоррор версия учёный фильм вызывать самый ...</td>\n",
       "      <td>-0.058252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>доллар дать сервис валюта продолжать падать до...</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>пять страна ес запрет выдача виза россиянин со...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>айтишник сделать общаться девушка вместо ия ба...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>женщина нью йорк париж билет прятаться туалет ...</td>\n",
       "      <td>0.121212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>реддитор видеокарта полноценный железнодорожны...</td>\n",
       "      <td>0.027027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>осень питер первый минус год центр погода фобо...</td>\n",
       "      <td>-0.045455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      processed_text  sent_score\n",
       "0  hugging face выпустить бесплатный курс ия аген...    0.076923\n",
       "1  кастомизировать ес киберстандарт samsung блоки...    0.000000\n",
       "2  банк россия снизить ключевой ставка это четвёр...   -0.037037\n",
       "3  топ хоррор версия учёный фильм вызывать самый ...   -0.058252\n",
       "4  доллар дать сервис валюта продолжать падать до...    0.062500\n",
       "5  пять страна ес запрет выдача виза россиянин со...    0.000000\n",
       "6  айтишник сделать общаться девушка вместо ия ба...    0.000000\n",
       "7  женщина нью йорк париж билет прятаться туалет ...    0.121212\n",
       "8  реддитор видеокарта полноценный железнодорожны...    0.027027\n",
       "9  осень питер первый минус год центр погода фобо...   -0.045455"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for df in (train, val, test):\n",
    "    df[\"sent_score\"] = df[\"processed_text\"].apply(lambda s: lexicon_score(s, pos_set, neg_set))\n",
    "\n",
    "train[[\"processed_text\", \"sent_score\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5bb3952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(DATA_DIR / \"data_train_with_sent.csv\", index=False)\n",
    "val.to_csv(DATA_DIR / \"data_val_with_sent.csv\", index=False)\n",
    "test.to_csv(DATA_DIR / \"data_test_with_sent.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
