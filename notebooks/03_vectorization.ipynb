{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67b4950",
   "metadata": {},
   "source": [
    "# Часть 3. Векторизация текста\n",
    "\n",
    "**Цель:** преобразовать предобработанные тексты в числовые признаки для последующего обучения моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0f08cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c485ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11eb8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = DATA_DIR / \"data_train.csv\"\n",
    "val_path   = DATA_DIR / \"data_val.csv\"\n",
    "test_path  = DATA_DIR / \"data_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c0a346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_path)\n",
    "val   = pd.read_csv(val_path)\n",
    "test  = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55cbad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (train, val, test):\n",
    "    df[\"processed_text\"] = df[\"processed_text\"].fillna(\"\").astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4ee6f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2420, 14), (428, 14), (503, 14))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e42c53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = train[\"processed_text\"].values\n",
    "X_val_text   = val[\"processed_text\"].values\n",
    "X_test_text  = test[\"processed_text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0669974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[\"target\"].values if \"target\" in train.columns else None\n",
    "y_val   = val[\"target\"].values if \"target\" in val.columns else None\n",
    "y_test  = test[\"target\"].values if \"target\" in test.columns else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf9cb124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2420, 428, 503)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_text), len(X_val_text), len(X_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b33a2f",
   "metadata": {},
   "source": [
    "## BoW\n",
    "BoW формирует вектор признаков как частоты слов, без учета порядка\n",
    "Используем только обучающую выборку для обучения векторизатора, затем применяем к val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6bf3c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7be9684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(\n",
    "    min_df=2,          # можно подкрутить, чтобы убрать редкие слова\n",
    "    max_df=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe798a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow = bow.fit_transform(X_train_text)\n",
    "X_val_bow   = bow.transform(X_val_text)\n",
    "X_test_bow  = bow.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b8a4ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6682"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61c1b1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2420, 6682), (428, 6682), (503, 6682))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow.shape, X_val_bow.shape, X_test_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dfa153",
   "metadata": {},
   "source": [
    "Сохранение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71cfea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VEC_DIR  = Path(\"../models/vectorizers\")\n",
    "FEAT_DIR = Path(\"../data/features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5eada59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\models\\\\vectorizers\\\\bow.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(bow, VEC_DIR / \"bow.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0ec585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(FEAT_DIR / \"X_train_bow.npz\", X_train_bow)\n",
    "sparse.save_npz(FEAT_DIR / \"X_val_bow.npz\",   X_val_bow)\n",
    "sparse.save_npz(FEAT_DIR / \"X_test_bow.npz\",  X_test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574fc773",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "TF-IDF снижает вклад слов, которые встречаются часто, и повышает вклад информативных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d42d41da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99c51e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    ngram_range=(1, 2)  # биграммы часто дают буст на текстах\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90343b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "X_val_tfidf   = tfidf.transform(X_val_text)\n",
    "X_test_tfidf  = tfidf.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d547e779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13202"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e0002f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2420, 13202)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb99485",
   "metadata": {},
   "source": [
    "Сохранение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cb8763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(tfidf, VEC_DIR / \"tfidf.joblib\")\n",
    "\n",
    "sparse.save_npz(FEAT_DIR / \"X_train_tfidf.npz\", X_train_tfidf)\n",
    "sparse.save_npz(FEAT_DIR / \"X_val_tfidf.npz\",   X_val_tfidf)\n",
    "sparse.save_npz(FEAT_DIR / \"X_test_tfidf.npz\",  X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c317690",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Word2Vec обучается на корпусе текстов и строит вектор для каждого слова.\n",
    "Для получения вектора текста используется усреднение векторов его слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c34fd849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09e21aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokens = [t.split() for t in X_train_text]\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=corpus_tokens,\n",
    "    vector_size=300,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    epochs=30,\n",
    "    sg=1  # skip-gram\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80d2a96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7092, 300)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.wv.key_to_index), w2v_model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ae64757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v(texts, model):\n",
    "    dim = model.vector_size\n",
    "    out = np.zeros((len(texts), dim), dtype=np.float32)\n",
    "\n",
    "    for i, txt in enumerate(texts):\n",
    "        tokens = txt.split()\n",
    "        vecs = [model.wv[w] for w in tokens if w in model.wv]\n",
    "        if vecs:\n",
    "            out[i] = np.mean(vecs, axis=0)\n",
    "        else:\n",
    "            out[i] = np.zeros(dim, dtype=np.float32)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "495c224e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2420, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_w2v = w2v(X_train_text, w2v_model)\n",
    "X_val_w2v   = w2v(X_val_text, w2v_model)\n",
    "X_test_w2v  = w2v(X_test_text, w2v_model)\n",
    "\n",
    "X_train_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7420b98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.85362082e-02 -3.02317172e-01 -1.22372098e-01  2.46591628e-01\n",
      " -2.78923631e-01 -6.26711845e-02  5.60295805e-02  5.07379174e-01\n",
      "  1.61740825e-01 -3.66258062e-02 -1.16450027e-01 -2.47824714e-01\n",
      " -1.67768728e-02  1.70084536e-02 -3.20238292e-01 -7.39222690e-02\n",
      " -3.75880390e-01 -1.10278532e-01  3.42577428e-01 -1.58759326e-01\n",
      "  3.22359465e-02 -4.88416478e-02  1.77635565e-01 -1.32499784e-01\n",
      " -8.26681554e-02  2.62693435e-01 -1.45722210e-01  1.14757076e-01\n",
      " -3.05073522e-02 -1.51523739e-01  1.39749408e-01 -9.01349783e-02\n",
      " -8.47139359e-02 -3.25300172e-02 -8.29679668e-02  1.98650092e-01\n",
      " -3.54386419e-02  2.39063948e-01  1.00757487e-01 -2.01146200e-01\n",
      " -4.94157746e-02  4.64006662e-02  1.12735324e-01 -1.35847358e-02\n",
      " -1.23815618e-01  1.17701907e-02  2.33827047e-02  1.90789416e-01\n",
      " -1.76305488e-01  2.03776091e-01 -1.00713842e-01 -2.33924817e-02\n",
      " -2.93975770e-01 -1.22901618e-01 -1.12255901e-01 -5.24049699e-02\n",
      "  5.67219779e-02  3.86181027e-01  1.46009251e-01  1.42199382e-01\n",
      " -2.16965050e-01 -1.13056928e-01 -1.44317120e-01 -8.43009055e-02\n",
      " -1.04979590e-01 -1.72766596e-01 -9.46654454e-02  1.55656323e-01\n",
      "  2.51852185e-01 -2.77818918e-01 -1.56918079e-01  3.71013969e-01\n",
      "  1.70745343e-01 -5.09131029e-02 -5.22666015e-02 -1.97067350e-01\n",
      " -3.13877076e-01 -1.93808764e-01 -2.53823221e-01 -1.74943581e-02\n",
      "  2.07695842e-01  1.62878595e-02  6.99219853e-02  3.05982769e-01\n",
      " -5.32641523e-02 -3.40530008e-01 -1.38452023e-01  1.12212978e-01\n",
      " -6.08069487e-02 -3.92101780e-02 -1.57989770e-01 -3.16801280e-01\n",
      "  2.55214330e-02  3.36817712e-01  1.68751612e-01 -1.12592410e-02\n",
      "  1.41066104e-01 -1.17255621e-01 -4.95302156e-02  2.22694367e-01\n",
      " -1.19103536e-01 -2.45719612e-01 -1.24640621e-01  2.36911684e-01\n",
      " -1.13407761e-01  7.71966800e-02 -1.76147670e-01  2.23010898e-01\n",
      " -4.62887257e-01  2.44529974e-02 -1.52879357e-01 -5.89248478e-01\n",
      "  6.35080859e-02  2.97836691e-01  2.28857607e-01  1.20611690e-01\n",
      "  2.95773238e-01 -5.73007762e-02  1.34450883e-01 -9.05560479e-02\n",
      " -1.64509304e-02 -1.49417147e-01  1.77181676e-01  1.22512504e-01\n",
      " -4.27520536e-02  8.00388083e-02  4.11360860e-02  3.71138811e-01\n",
      " -1.35021955e-01  1.20734721e-02 -2.29992419e-01  6.03918508e-02\n",
      "  2.42942899e-01 -2.25774035e-01  2.70562042e-02 -1.38505355e-01\n",
      " -3.11073363e-02 -5.37191369e-02 -2.46490300e-01 -1.95785254e-01\n",
      "  1.62887331e-02 -1.59734428e-01 -1.75401464e-01  8.94435775e-03\n",
      "  3.01226646e-01 -1.83858603e-01 -1.35018423e-01  2.07108259e-01\n",
      "  2.85462141e-01  6.43137693e-02 -1.04938947e-01 -1.11372590e-01\n",
      " -1.30935498e-02 -3.60387981e-01 -1.19431593e-01 -1.67346448e-01\n",
      " -3.40053171e-01 -7.79695883e-02  1.48486197e-02  3.72765988e-01\n",
      " -2.63001561e-01  1.51098132e-01 -4.43088599e-02  3.91470551e-01\n",
      " -3.59719992e-02  7.66169280e-02 -1.27571568e-01  1.75102632e-02\n",
      "  3.96018662e-02 -3.77878547e-02  1.01237528e-01 -9.40058231e-02\n",
      "  3.14859807e-01  7.25311041e-02 -1.46053031e-01 -2.37866089e-01\n",
      "  2.82950580e-01  2.31728554e-01  2.09811047e-01 -1.42071858e-01\n",
      " -2.42795408e-01  4.14801091e-02 -1.10210031e-01 -3.53635609e-01\n",
      " -8.67874473e-02 -5.15980236e-02  6.45577982e-02  8.02377611e-02\n",
      " -1.45212933e-01 -3.86174530e-01 -6.33385926e-02 -1.94655787e-02\n",
      " -1.39567450e-01  1.74300447e-01  3.22362501e-03  4.06503350e-01\n",
      " -1.37177423e-01 -2.65401334e-01  2.03812659e-01 -1.14485621e-01\n",
      " -1.05935894e-01 -2.63700206e-02 -8.19929913e-02 -4.02694605e-02\n",
      "  1.53040821e-02  2.81354655e-02 -1.68560669e-01 -1.61317717e-02\n",
      "  1.79604888e-01 -5.89520484e-02 -1.00004263e-01  1.08399145e-01\n",
      " -7.77800381e-02 -3.42398822e-01  3.33587348e-01  8.89397115e-02\n",
      "  5.31066209e-02 -3.60653937e-01 -7.15303272e-02  1.87609076e-01\n",
      " -7.99175799e-02 -9.37828049e-03  1.32149205e-01 -2.79041138e-02\n",
      " -4.54448350e-02 -1.53129593e-01  1.84021696e-01 -1.39476627e-01\n",
      " -3.62039894e-01  1.29323388e-02  9.82132182e-02 -1.43899903e-01\n",
      " -8.17719251e-02  1.79584220e-01  1.53499466e-04 -3.73000145e-01\n",
      "  2.46681720e-01  1.31798638e-02  4.10145044e-01 -1.94521621e-01\n",
      " -9.86729041e-02  1.01619428e-02 -3.31937879e-01 -1.01803966e-01\n",
      " -1.77326813e-01 -6.91218600e-02 -1.49242327e-01  3.41355875e-02\n",
      " -1.40137985e-01 -1.32897422e-01  2.17324808e-01  1.12920618e-02\n",
      "  8.58906657e-02 -3.28752309e-01 -8.82913992e-02 -1.73848066e-02\n",
      "  3.66464227e-01  7.89300259e-03  7.35552534e-02  1.47308661e-02\n",
      "  2.99738169e-01  2.25885123e-01 -9.64476094e-02 -2.05480665e-01\n",
      "  1.84666008e-01  7.22296312e-02 -2.26170912e-01 -5.71137741e-02\n",
      "  2.02426821e-01 -1.95043221e-01  2.18136445e-01  2.16357205e-02\n",
      " -1.43435284e-01  1.19017363e-01  2.15908557e-01 -4.42118645e-02\n",
      "  5.53274760e-03  8.81803036e-03  7.65764490e-02  2.55097628e-01\n",
      " -2.20120493e-02  2.34713182e-02 -1.91527214e-02 -3.23610723e-01\n",
      " -1.50011107e-01 -5.28603084e-02  6.63662935e-03  1.46463737e-01\n",
      "  2.37639204e-01  4.37493026e-01  3.12477112e-01  4.90444392e-01\n",
      "  3.00455481e-01  1.36455223e-01  3.17387342e-01  2.63775855e-01\n",
      "  6.24470040e-02 -1.47996128e-01  7.37504885e-02  5.92521625e-03]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv['tesla'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "52742a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('intelligence', 0.6442187428474426),\n",
       " ('airtag', 0.5924407839775085),\n",
       " ('watch', 0.5889187455177307),\n",
       " ('homepod', 0.571516752243042),\n",
       " ('линейка', 0.5145259499549866),\n",
       " ('fcc', 0.5070827603340149),\n",
       " ('подписывать', 0.5065293908119202),\n",
       " ('сентябрьский', 0.5042416453361511),\n",
       " ('music', 0.5007003545761108),\n",
       " ('макроданные', 0.49501731991767883)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ближайшие слова\n",
    "w2v_model.wv.most_similar('apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f957023d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('samsung', 0.49828338623046875),\n",
       " ('разъём', 0.4805389642715454),\n",
       " ('шифроваться', 0.4763013422489166),\n",
       " ('pixel', 0.4714619815349579),\n",
       " ('canon', 0.4686709940433502),\n",
       " ('складный', 0.46739572286605835),\n",
       " ('троттлинг', 0.4616629481315613),\n",
       " ('usb', 0.45923134684562683),\n",
       " ('фактор', 0.45651426911354065),\n",
       " ('зарядка', 0.4533364176750183)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ближайшие слова\n",
    "w2v_model.wv.most_similar('смартфон')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "76111e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.33037722)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# косинусная близость\n",
    "w2v_model.wv.similarity('apple', 'iphone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f5a5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIR  = Path(\"../models/embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cefe706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(str(EMB_DIR / \"word2vec.model\"))\n",
    "\n",
    "np.save(FEAT_DIR / \"X_train_w2v.npy\", X_train_w2v)\n",
    "np.save(FEAT_DIR / \"X_val_w2v.npy\",   X_val_w2v)\n",
    "np.save(FEAT_DIR / \"X_test_w2v.npy\",  X_test_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5142e32",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "Используем мультиязычную модель, чтобы корректно работать с русским текстом.\n",
    "Извлекаем вектор текста:\n",
    "- cls: вектор токена [CLS]\n",
    "- mean: среднее по всем токенам последнего слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30af70f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arina\\VS Code Projects\\social-media-analytics\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3b8bedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4edaf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-multilingual-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1ca23b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arina\\VS Code Projects\\social-media-analytics\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Arina\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 299.91it/s, Materializing param=pooler.dense.weight]                               \n",
      "BertModel LOAD REPORT from: bert-base-multilingual-cased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert = BertModel.from_pretrained(MODEL_NAME).to(device)\n",
    "bert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1880563c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2af43e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bert_embed_texts(texts, pooling=\"cls\", batch_size=16, max_len=128):\n",
    "#     all_vecs = []\n",
    "#     with torch.no_grad():\n",
    "#         for i in range(0, len(texts), batch_size):\n",
    "#             batch = list(texts[i:i+batch_size])\n",
    "#             enc = tokenizer(\n",
    "#                 batch,\n",
    "#                 return_tensors=\"pt\",\n",
    "#                 padding=True,\n",
    "#                 truncation=True,\n",
    "#                 max_length=max_len\n",
    "#             ).to(device)\n",
    "\n",
    "#             out = bert(**enc).last_hidden_state \n",
    "\n",
    "#             if pooling == \"cls\":\n",
    "#                 vec = out[:, 0, :]  # [B, H]\n",
    "#             elif pooling == \"mean\":\n",
    "#                 attn = enc[\"attention_mask\"].unsqueeze(-1)  # [B, T, 1]\n",
    "#                 summed = (out * attn).sum(dim=1)\n",
    "#                 counts = attn.sum(dim=1).clamp(min=1)\n",
    "#                 vec = summed / counts\n",
    "#             else:\n",
    "#                 raise ValueError(\"pooling must be 'cls' or 'mean'\")\n",
    "\n",
    "#             all_vecs.append(vec.cpu().numpy())\n",
    "\n",
    "#     return np.vstack(all_vecs)\n",
    "\n",
    "# # пример на маленьком куске (можешь убрать ограничение потом)\n",
    "# X_train_bert = bert_embed_texts(X_train_text[:500], pooling=\"cls\", batch_size=16, max_len=128)\n",
    "# X_train_bert.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e1ae2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_embeddings(texts, batch_size=16, max_len=128, pooling=\"cls\"):\n",
    "    \"\"\"texts: list/np.array of strings -> np.array [N, hidden_size]\"\"\"\n",
    "    vecs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = list(texts[i:i+batch_size])\n",
    "\n",
    "            enc = tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            last_hidden = bert(**enc).last_hidden_state  # [B, T, H]\n",
    "\n",
    "            if pooling == \"cls\":\n",
    "                emb = last_hidden[:, 0, :]  # [B, H]\n",
    "            elif pooling == \"mean\":\n",
    "                # среднее только по “реальным” токенам (без паддинга)\n",
    "                mask = enc[\"attention_mask\"].unsqueeze(-1)  # [B, T, 1]\n",
    "                emb = (last_hidden * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "            else:\n",
    "                raise ValueError(\"pooling must be 'cls' or 'mean'\")\n",
    "\n",
    "            vecs.append(emb.cpu().numpy())\n",
    "\n",
    "    return np.vstack(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9134c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bert = bert_embeddings(X_train_text, pooling=\"cls\", batch_size=16, max_len=128)\n",
    "X_val_bert   = bert_embeddings(X_val_text,   pooling=\"cls\", batch_size=16, max_len=128)\n",
    "X_test_bert  = bert_embeddings(X_test_text,  pooling=\"cls\", batch_size=16, max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "339a8662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2420, 768) (428, 768) (503, 768)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_bert.shape, X_val_bert.shape, X_test_bert.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec0af904",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(FEAT_DIR / \"X_train_bert_cls.npy\", X_train_bert)\n",
    "np.save(FEAT_DIR / \"X_val_bert_cls.npy\",   X_val_bert)\n",
    "np.save(FEAT_DIR / \"X_test_bert_cls.npy\",  X_test_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b0e92",
   "metadata": {},
   "source": [
    "## Оценка тональности по тональному словарю \n",
    "\n",
    "Дополнительно оцениваем тональность текста поста по словарю/\n",
    "В качестве интерпретируемого базового признака использован русскоязычный тональный словарь RuSentiLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b262ed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEX_PATH = Path(\"../data/external/rusentilex.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5eb6c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rusentilex_txt(path):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Нет файла: {path.resolve()}\")\n",
    "\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"!\"):\n",
    "                continue\n",
    "            parts = [p.strip() for p in line.split(\",\")]\n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            rows.append(parts[:5])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"word\", \"pos\", \"lemma\", \"sentiment\", \"source\"])\n",
    "\n",
    "    df[\"lemma\"] = df[\"lemma\"].astype(str).str.lower().str.strip()\n",
    "    df[\"sentiment\"] = df[\"sentiment\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "    df = df[~df[\"lemma\"].str.contains(r\"\\s+\", regex=True)]\n",
    "    pos_set = set(df.loc[df[\"sentiment\"] == \"positive\", \"lemma\"])\n",
    "    neg_set = set(df.loc[df[\"sentiment\"] == \"negative\", \"lemma\"])\n",
    "\n",
    "    return pos_set, neg_set, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "221571d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: 2790 neg: 7867\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>lemma</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>аборт</td>\n",
       "      <td>Noun</td>\n",
       "      <td>аборт</td>\n",
       "      <td>negative</td>\n",
       "      <td>fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>абортивный</td>\n",
       "      <td>Adj</td>\n",
       "      <td>абортивный</td>\n",
       "      <td>negative</td>\n",
       "      <td>fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>абракадабра</td>\n",
       "      <td>Noun</td>\n",
       "      <td>абракадабра</td>\n",
       "      <td>negative</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>абсурд</td>\n",
       "      <td>Noun</td>\n",
       "      <td>абсурд</td>\n",
       "      <td>negative</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>абсурдность</td>\n",
       "      <td>Noun</td>\n",
       "      <td>абсурдность</td>\n",
       "      <td>negative</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word   pos        lemma sentiment   source\n",
       "0        аборт  Noun        аборт  negative     fact\n",
       "1   абортивный   Adj   абортивный  negative     fact\n",
       "2  абракадабра  Noun  абракадабра  negative  opinion\n",
       "3       абсурд  Noun       абсурд  negative  opinion\n",
       "4  абсурдность  Noun  абсурдность  negative  opinion"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_set, neg_set, rusenti_df = load_rusentilex_txt(LEX_PATH)\n",
    "\n",
    "print(\"pos:\", len(pos_set), \"neg:\", len(neg_set))\n",
    "rusenti_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d76ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicon_score(text, pos_set, neg_set):\n",
    "    toks = str(text).split() \n",
    "    if not toks:\n",
    "        return 0.0\n",
    "    pos_cnt = sum(t in pos_set for t in toks)\n",
    "    neg_cnt = sum(t in neg_set for t in toks)\n",
    "    return (pos_cnt - neg_cnt) / max(1, len(toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb09a3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_text</th>\n",
       "      <th>sent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple новый mac следующий неделя вица президен...</td>\n",
       "      <td>-0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>имб вайбкодер рабочий промт долгий проект gpt ...</td>\n",
       "      <td>-0.003546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>мощный генератор видео veo открыть пользовател...</td>\n",
       "      <td>0.050847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>внезапно приложение sora лидер американский ap...</td>\n",
       "      <td>0.054054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>valve сразу игровой гаджет консоль vr очки гей...</td>\n",
       "      <td>0.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>apple новый macbook air чип главное раз быстры...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>google бюджетный pixel смартфон получить сразу...</td>\n",
       "      <td>-0.027778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>курсовой минута grok появиться генерация pdf ф...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>поездка сапсан новогодний ночь цена распростра...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>проблема будущее мужчина попытаться угнать бес...</td>\n",
       "      <td>-0.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      processed_text  sent_score\n",
       "0  apple новый mac следующий неделя вица президен...   -0.038462\n",
       "1  имб вайбкодер рабочий промт долгий проект gpt ...   -0.003546\n",
       "2  мощный генератор видео veo открыть пользовател...    0.050847\n",
       "3  внезапно приложение sora лидер американский ap...    0.054054\n",
       "4  valve сразу игровой гаджет консоль vr очки гей...    0.034483\n",
       "5  apple новый macbook air чип главное раз быстры...    0.000000\n",
       "6  google бюджетный pixel смартфон получить сразу...   -0.027778\n",
       "7  курсовой минута grok появиться генерация pdf ф...    0.000000\n",
       "8  поездка сапсан новогодний ночь цена распростра...    0.000000\n",
       "9  проблема будущее мужчина попытаться угнать бес...   -0.090909"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for df in (train, val, test):\n",
    "    df[\"sent_score\"] = df[\"processed_text\"].apply(lambda s: lexicon_score(s, pos_set, neg_set))\n",
    "\n",
    "train[[\"processed_text\", \"sent_score\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bb3952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(DATA_DIR / \"data_train_with_sent.csv\", index=False)\n",
    "val.to_csv(DATA_DIR / \"data_val_with_sent.csv\", index=False)\n",
    "test.to_csv(DATA_DIR / \"data_test_with_sent.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
